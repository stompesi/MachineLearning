{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from numpy import *\n",
    "import re\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 모든 문서에 있는 모든 유일한 단어 목록을 생성한다.\n",
    "def create_vocab_list(data_set):\n",
    "    # 비어있는 집합 생성\n",
    "    vocab_set = set([]) \n",
    "    for document in data_set:\n",
    "        # 연산자 | 는 두 개의 집합 유형의 변수를 합치는 데 사용하는 연산자.\n",
    "        # 두 개의 집합 통합 생성\n",
    "        # ex) {'a', 'b', 'c'} | {'c', 'd', 'e'} \n",
    "        #     -> {'a', 'b', 'c', 'd', 'e'} \n",
    "        vocab_set = vocab_set | set(document)\n",
    "    return list(vocab_set)\n",
    "\n",
    "# 주어진 문서 내에 어휘 목록에 있는 단어가 존재하는지 아닌지를 표현하기 위해 어휘 목록, 문서, \n",
    "# 1과 0의 출력 벡터를 사용한다.\n",
    "def set_of_words_2_vec(vocab_list, input_set):\n",
    "    # 모두 0인 벡터 생성\n",
    "    return_vec = [0] * len(vocab_list) \n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            print (\"the word: %s is not in my Vocablulary!\" % word)\n",
    "    return return_vec\n",
    "\n",
    "def train(train_matrix, train_category):\n",
    "    # 문서의 수\n",
    "    num_train_docs = len(train_matrix)\n",
    "    \n",
    "    # 한문서의 최대 단어수\n",
    "    num_words = len(train_matrix[0])\n",
    "    \n",
    "    # 폭력적인 문서의 확률 계산\n",
    "    # 1의 값이 폭력적이기 때문에 train_category를 합하면 폭력적인 문서의 수가 나온다.\n",
    "    # 폭력적인 문서 수 / 전체 문서 수\n",
    "    # 현재는 분류 항목이 두개 이기때문에 가능 / 분류 항목이 2개 이상이면 이부분을 수정해야 한다.\n",
    "    p_abusive = sum(train_category) / float(num_train_docs)\n",
    "    \n",
    "    # 확률 초기화\n",
    "    # zerors: num_words(인자값) 수 만큼 0. 배열 만듬\n",
    "    p_0_num = ones(num_words) \n",
    "    p_1_num = ones(num_words)\n",
    "    p_0_denom = 2.0\n",
    "    p_1_denom = 2.0\n",
    "    \n",
    "    # 벡터 추가 \n",
    "    for i in range(num_train_docs):\n",
    "        if train_category[i] == 1:\n",
    "            p_1_num += train_matrix[i]\n",
    "            p_1_denom += sum(train_matrix[i])\n",
    "        else:\n",
    "            p_0_num += train_matrix[i]\n",
    "            p_0_denom += sum(train_matrix[i])\n",
    "\n",
    "    # 원소 나누기\n",
    "    p_1_vect = log(p_1_num / p_1_denom)\n",
    "    p_0_vect = log(p_0_num / p_1_denom)\n",
    "    return p_0_vect, p_1_vect, p_abusive\n",
    "\n",
    "def classify(vec_2_classify, p_0_vec, p_1_vec, p_class_1):\n",
    "    # 원소 곱하기\n",
    "    # 방식은 두 벡터의 첫 번째 원소들을 곱한 뒤, 두 번째 원소들을 곱하고, 이런식으로 계속해서 끝까지 곱해 가는 방식이다. \n",
    "    # 그런다음, 어휘집에 있는 모든 단어들에 대한 값을 더하고, 분류 항목의 로그 확률에 더한다.\n",
    "    # \n",
    "    p1 = sum(vec_2_classify * p_1_vec) + log(p_class_1)\n",
    "    p0 = sum(vec_2_classify * p_0_vec) + log(1.0 - p_class_1)\n",
    "    \n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('data.csv', 'r') as f:\n",
    "    reader = csv.reader(f) \n",
    "    for row in reader:\n",
    "        row[0] = row[0].strip()\n",
    "        data.append(row)\n",
    "data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 큰 문자열을 처리하며, 문자열 리스트로 텍스트를 구문 분석한다.\n",
    "# 길이가 1개 이하인 단어는 탈락\n",
    "def text_parse(big_string):\n",
    "    list_of_tokens = kkma.nouns(big_string)\n",
    "    return [token for token in list_of_tokens if len(token) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조합이안됩니다처음부터 끝~~~~까지\n",
      "['조합', '처음']\n"
     ]
    }
   ],
   "source": [
    "kkma = Kkma()\n",
    "doc_list = []\n",
    "class_list = []\n",
    "full_text = []\n",
    "\n",
    "print (data[3][0])\n",
    "print (text_parse(data[3][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, 200):\n",
    "    word_list = text_parse(data[i][0])\n",
    "    doc_list.append(word_list)\n",
    "    full_text.extend(word_list)\n",
    "    class_list.append(int(data[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 검사 집합과 훈련집합을 생성한다.\n",
    "vocab_list = create_vocab_list(doc_list)\n",
    "training_set = list(range(200))\n",
    "test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 30개를 랜덤으로 생성해서 훈련용 데이터와 검사용데이터로 나눈다.\n",
    "for i in range(30):\n",
    "    rand_index = int(random.uniform(0, len(training_set)))\n",
    "    test_set.append(training_set[rand_index])\n",
    "    del(training_set[rand_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_matrix = []\n",
    "train_classes = []\n",
    "# 검사 집합에 있는 모든 아이템을 반복하며, 각 이메일과 어휘집에 있는 단어들로부터 set_of_words_2_vec를 사용하여 단어 벡터를 생성한다.\n",
    "for doc_index in training_set:\n",
    "#     train_matrix.append(set_of_words_2_vec(vocab_list, doc_list[doc_index]))\n",
    "    train_matrix.append(bag_of_words_2_vec_MN(vocab_list, doc_list[doc_index]))\n",
    "    train_classes.append(class_list[doc_index])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 단어들은 분류에 필요한 확률을 계산하기위해 train 데이터를 사용한다.\n",
    "p_0_v, p_1_v, p_spam = train(array(train_matrix), array(train_classes))\n",
    "error_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error rate is:  0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "# 검사 집합을 반복하고 검사 집합 내에서 각 이메일을 분류한다. \n",
    "# 분류가 제데로 되지 않았다면 오류 개수를 증가한다.\n",
    "for doc_index in test_set:\n",
    "#     word_vector = set_of_words_2_vec(vocab_list, doc_list[doc_index])\n",
    "    word_vector = bag_of_words_2_vec_MN(vocab_list, doc_list[doc_index])\n",
    "    if classify(array(word_vector), p_0_v, p_1_v, p_spam) != class_list[doc_index]:\n",
    "        error_count += 1\n",
    "print ('The error rate is: ', float(error_count) / len(test_set))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
