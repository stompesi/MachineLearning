{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_set():\n",
    "    # 달마시안 애호가 전자 게시판에서 가져온 문서 집합을 토큰화 한 것\n",
    "    posting_list = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "\n",
    "    # 1: 폭력적임, 0: 폭력적이지 않음\n",
    "    # 텍스트에 사람이 직접 분류 항목 표시를 붙이고, 폭력적인 게시물을 자동적으로 감지하는 프로그램을 훈련하는데 사용한다.\n",
    "    class_vec = [0,1,0,1,0,1] \n",
    "    return posting_list, class_vec\n",
    "\n",
    "# 모든 문서에 있는 모든 유일한 단어 목록을 생성한다.\n",
    "def create_vocab_list(data_set):\n",
    "    # 비어있는 집합 생성\n",
    "    vocab_set = set([]) \n",
    "    for document in data_set:\n",
    "        # 연산자 | 는 두 개의 집합 유형의 변수를 합치는 데 사용하는 연산자.\n",
    "        # 두 개의 집합 통합 생성\n",
    "        # ex) {'a', 'b', 'c'} | {'c', 'd', 'e'} \n",
    "        #     -> {'a', 'b', 'c', 'd', 'e'} \n",
    "        vocab_set = vocab_set | set(document)\n",
    "    return list(vocab_set)\n",
    "\n",
    "# 주어진 문서 내에 어휘 목록에 있는 단어가 존재하는지 아닌지를 표현하기 위해 어휘 목록, 문서, \n",
    "# 1과 0의 출력 벡터를 사용한다.\n",
    "def set_of_words_2_vec(vocab_list, input_set):\n",
    "    # 모두 0인 벡터 생성\n",
    "    return_vec = [0] * len(vocab_list) \n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            print (\"the word: %s is not in my Vocablulary!\" % word)\n",
    "    return return_vec\n",
    "\n",
    "# Test\n",
    "list_o_posts, list_classes = load_data_set()\n",
    "my_vocab_list = create_vocab_list(list_o_posts)\n",
    "my_vocab_list\n",
    "set_of_words_2_vec(my_vocab_list, list_o_posts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def train(train_matrix, train_category):\n",
    "    # 문서의 수\n",
    "    num_train_docs = len(train_matrix)\n",
    "    \n",
    "    # 한문서의 최대 단어수\n",
    "    num_words = len(train_matrix[0])\n",
    "    \n",
    "    # 폭력적인 문서의 확률 계산\n",
    "    # 1의 값이 폭력적이기 때문에 train_category를 합하면 폭력적인 문서의 수가 나온다.\n",
    "    # 폭력적인 문서 수 / 전체 문서 수\n",
    "    # 현재는 분류 항목이 두개 이기때문에 가능 / 분류 항목이 2개 이상이면 이부분을 수정해야 한다.\n",
    "    p_abusive = sum(train_category) / float(num_train_docs)\n",
    "    \n",
    "    # 확률 초기화\n",
    "    # zerors: num_words(인자값) 수 만큼 0. 배열 만듬\n",
    "    p_0_num = ones(num_words) \n",
    "    p_1_num = ones(num_words)\n",
    "    p_0_denom = 2.0\n",
    "    p_1_denom = 2.0\n",
    "    \n",
    "    # 벡터 추가 \n",
    "    for i in range(num_train_docs):\n",
    "        if train_category[i] == 1:\n",
    "            p_1_num += train_matrix[i]\n",
    "            p_1_denom += sum(train_matrix[i])\n",
    "        else:\n",
    "            p_0_num += train_matrix[i]\n",
    "            p_0_denom += sum(train_matrix[i])\n",
    "\n",
    "    # 원소 나누기\n",
    "    p_1_vect = log(p_1_num / p_1_denom)\n",
    "    p_0_vect = log(p_0_num / p_1_denom)\n",
    "    return p_0_vect, p_1_vect, p_abusive\n",
    "\n",
    "# Test\n",
    "list_o_posts, list_classes = load_data_set()\n",
    "my_vocab_list = create_vocab_list(list_o_posts)\n",
    "train_matrix = []\n",
    "# 리스트 단어 벡터로 채운다\n",
    "for post_in_doc in list_o_posts:\n",
    "    train_matrix.append(set_of_words_2_vec(my_vocab_list, post_in_doc))\n",
    "p_0_v, p_1_v, p_ab = train(train_matrix, list_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classfied as:  0\n",
      "['stupid', 'garbage'] classified ad:  1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "vec_2_classify: 분류를 위한 벡터\n",
    "p_0_v, p_1_v, p_ab: train()에서 계산된 확률\n",
    "\"\"\"\n",
    "def classify(vec_2_classify, p_0_vec, p_1_vec, p_class_1):\n",
    "    # 원소 곱하기\n",
    "    # 방식은 두 벡터의 첫 번째 원소들을 곱한 뒤, 두 번째 원소들을 곱하고, 이런식으로 계속해서 끝까지 곱해 가는 방식이다. \n",
    "    # 그런다음, 어휘집에 있는 모든 단어들에 대한 값을 더하고, 분류 항목의 로그 확률에 더한다.\n",
    "    # \n",
    "    p1 = sum(vec_2_classify * p_1_vec) + log(p_class_1)\n",
    "    p0 = sum(vec_2_classify * p_0_vec) + log(1.0 - p_class_1)\n",
    "    \n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def testing():\n",
    "    list_o_posts, list_classes = load_data_set()\n",
    "    my_vocab_list = create_vocab_list(list_o_posts)\n",
    "    \n",
    "    train_matrix = []\n",
    "    \n",
    "    for post_in_doc in list_o_posts:\n",
    "        train_matrix.append(set_of_words_2_vec(my_vocab_list, post_in_doc))\n",
    "    \n",
    "    p_0_v, p_1_v, p_ab = train(array(train_matrix), array(list_classes))\n",
    "    \n",
    "    test_entry = ['love', 'my', 'dalmation']\n",
    "    this_doc = array(set_of_words_2_vec(my_vocab_list, test_entry))\n",
    "    \n",
    "    print (test_entry, 'classfied as: ', classify(this_doc, p_0_v, p_1_v, p_ab))\n",
    "    \n",
    "    test_entry = ['stupid', 'garbage']\n",
    "    this_doc = array(set_of_words_2_vec(my_vocab_list, test_entry))\n",
    "    \n",
    "    print (test_entry, 'classified ad: ', classify(this_doc, p_0_v, p_1_v, p_ab))\n",
    "\n",
    "#Test\n",
    "\n",
    "testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M.L.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중복단어 모델 \n",
    "def bag_of_words_2_vec_MN(vocab_list, input_set):\n",
    "    return_vec = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)] += 1\n",
    "    return return_vec\n",
    "\n",
    "# Test\n",
    "my_sent = ' This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "my_sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 구두점 제거\n",
    "reg_ex = re.compile('\\\\W*')\n",
    "list_of_tokens = reg_ex.split(my_sent)\n",
    "\n",
    "# 빈 문자열 제거 및 소문자 변환 (문자변환: lower, upper)\n",
    "\n",
    "[tok.lower() for tok in list_of_tokens if len(tok) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "email_text = open('ham/6.txt').read()\n",
    "list_of_tokens = reg_ex.split(email_text)\n",
    "list_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제 index: 15\n",
      "삭제 index: 48\n",
      "삭제 index: 28\n",
      "삭제 index: 8\n",
      "삭제 index: 27\n",
      "삭제 index: 41\n",
      "삭제 index: 29\n",
      "삭제 index: 29\n",
      "삭제 index: 10\n",
      "삭제 index: 21\n",
      "The error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "# 큰 문자열을 처리하며, 문자열 리스트로 텍스트를 구문 분석한다.\n",
    "# 길이가 2개 이하인 단어는 탈락시키며, 모든 단어를 소문자로 변환한다.\n",
    "def text_parse(big_string):\n",
    "    import re\n",
    "    list_of_tokens = re.split(r'\\W*', big_string)\n",
    "    return [tok.lower() for tok in list_of_tokens if len(tok) > 2]\n",
    "\n",
    "# 나이브 베이스 스팸 분류기를 자동화한다.\n",
    "def spam_test():\n",
    "    doc_list = []\n",
    "    class_list = []\n",
    "    full_text = []\n",
    "    \n",
    "    # 스팸과 햄의 텍스트 파일을 단어 리스트로 불러온다.\n",
    "    for i in range(1, 26):\n",
    "        word_list = text_parse(open('spam/%d.txt' % i).read())\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(1)\n",
    "        \n",
    "        word_list = text_parse(open('ham/%d.txt' % i).read())\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(0)\n",
    "    \n",
    "    # 검사 집합과 훈련집합을 생성한다.\n",
    "    vocab_list = create_vocab_list(doc_list)\n",
    "    training_set = list(range(50))\n",
    "    test_set = []\n",
    "\n",
    "    # 10개를 랜덤으로 생성해서 훈련용 데이터와 검사용데이터로 나눈다.\n",
    "    for i in range(10):\n",
    "        rand_index = int(random.uniform(0, len(training_set)))\n",
    "        test_set.append(training_set[rand_index])\n",
    "        print ('삭제 index: %d' % rand_index)\n",
    "        del(training_set[rand_index])\n",
    "    \n",
    "    train_matrix = []\n",
    "    train_classes = []\n",
    "    # 검사 집합에 있는 모든 아이템을 반복하며, 각 이메일과 어휘집에 있는 단어들로부터 set_of_words_2_vec를 사용하여 단어 벡터를 생성한다.\n",
    "    for doc_index in training_set:\n",
    "        train_matrix.append(set_of_words_2_vec(vocab_list, doc_list[doc_index]))\n",
    "        train_classes.append(class_list[doc_index])\n",
    "\n",
    "    # 단어들은 분류에 필요한 확률을 계산하기위해 train 데이터를 사용한다.\n",
    "    p_0_v, p_1_v, p_spam = train(array(train_matrix), array(train_classes))\n",
    "    error_count = 0\n",
    "    \n",
    "    # 검사 집합을 반복하고 검사 집합 내에서 각 이메일을 분류한다. \n",
    "    # 분류가 제데로 되지 않았다면 오류 개수를 증가한다.\n",
    "    for doc_index in test_set:\n",
    "        word_vector = set_of_words_2_vec(vocab_list, doc_list[doc_index])\n",
    "        if classify(array(word_vector), p_0_v, p_1_v, p_spam) != class_list[doc_index]:\n",
    "            error_count += 1\n",
    "    print ('The error rate is: ', float(error_count) / len(test_set))    \n",
    "# Test\n",
    "spam_test()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 나이브 베이스를 사용하여 개인 광고에 포함된 지역 특색 도출하기 \n",
    "# 다른 지역에 있는 사람들은 다른 단어를 사용한다는 것을 확인할 수 있을 것이다.\n",
    "\n",
    "import feedparser\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "len(ny['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feedparser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9f4878bb96bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://newyork.craigslist.org/stp/index/rss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0msf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://sfbay.craigslist.org/stp/index.rss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mvocab_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_SF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_NY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feedparser' is not defined"
     ]
    }
   ],
   "source": [
    "# 어휘집에 있는 모든 단어를 훑으면서 단어가 텍스트에 얼마나 많이 출현하는지 센다.\n",
    "# 딕셔너리에는 빈도가 가장 높은 것부터 가장 낮은 순으로 정렬되어 있으며, 이 중 상위 100개의 단어를 반환한다.\n",
    "def calc_most_freq(vocab_list, full_text):\n",
    "    import operator\n",
    "    freq_dict = {}\n",
    "    # 발생 빈도 계산\n",
    "    for token in vocab_list:\n",
    "        freq_dict[token] = full_text.count(token)\n",
    "    \n",
    "    sorted_freq = sorted(freq_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    # 빈도를 바꾸어 계산해보자\n",
    "    return sorted_freq[:30]\n",
    "\n",
    "\"\"\"\n",
    "feed는 외부에서 블러와야 한다.\n",
    "spamTest와 비슷하다. \n",
    "다른것은 파일 대신 피드를 처리한다는 것과 100개의 단어를 얻어 이 단어들을 제거하기 위해 cal_most_freq를 호출한다는 것이다.\n",
    "\"\"\"\n",
    "def local_words(feed1, feed2):\n",
    "    import feedparser\n",
    "    doc_list = []\n",
    "    class_list = []\n",
    "    full_text = []\n",
    "    min_len = min(len(feed1['entries']), len(feed2['entries']))\n",
    "    for i in range(min_len):\n",
    "        # 한 번에 하나의 피드 처리\n",
    "        word_list = text_parse(feed1['entries'][i]['summary'])\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(1)\n",
    "        \n",
    "        word_list = text_parse(feed2['entries'][i]['summary'])\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(0)\n",
    "    \n",
    "\n",
    "    vocab_list = create_vocab_list(doc_list)\n",
    "    top_30_words = calc_most_freq(vocab_list, full_text)\n",
    "    \n",
    "    # 발생 빈도가 가장 많은 단어 삭제\n",
    "    \"\"\"\n",
    "    게시물에 있는 단어중 상위 30개의 단어가 사용된 모든 단어의 30% 정도를 차지한다는 것이다.\n",
    "    즉, 사용된 전체 단어 중 몇 개의 단어가 텍스트 문서의 많은 부분을 차지한다는 것이다.\n",
    "    \"\"\"\n",
    "    for pair_w in top_30_words:\n",
    "        if pair_w[0] in vocab_list:\n",
    "            vocab_list.remove(pair_w[0])\n",
    "\n",
    "    training_set = list(range(2 * min_len))\n",
    "    test_set = []\n",
    "    \n",
    "    for i in range(20):\n",
    "        rand_index = int(random.uniform(0, len(training_set)))\n",
    "        test_set.append(training_set[rand_index])\n",
    "        del(training_set[rand_index])\n",
    "    \n",
    "    train_matrix = []\n",
    "    train_classes = []\n",
    "    \n",
    "    for doc_index in training_set:\n",
    "        train_matrix.append(bag_of_words_2_vec_MN(vocab_list, doc_list[doc_index]))\n",
    "        train_classes.append(class_list[doc_index])\n",
    "    \n",
    "    p_0_v, p_1_v, p_sapm = train(array(train_matrix), array(train_classes))\n",
    "    error_count = 0\n",
    "    \n",
    "    for doc_index in test_set:\n",
    "        word_vector = bag_of_words_2_vec_MN(vocab_list, doc_list[doc_index])\n",
    "        if classify(array(word_vector), p_0_v, p_1_v, p_spam) != class_list[doc_index]:\n",
    "            error_count += 1\n",
    "    \n",
    "    print ('The error rate is: ', float(error_count) / len(test_set))\n",
    "    return vocab_list, p_0_v, p_1_v\n",
    "           \n",
    "# Test\n",
    "\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index/rss')\n",
    "sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "vocab_list, p_SF, p_NY = local_words(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error rate is:  0.45\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "text\n",
      "need\n",
      "fun\n",
      "wanna\n",
      "want\n",
      "are\n",
      "from\n",
      "share\n",
      "let\n",
      "contact\n",
      "what\n",
      "would\n",
      "hit\n",
      "send\n",
      "fit\n",
      "single\n",
      "lets\n",
      "drink\n",
      "our\n",
      "title\n",
      "various\n",
      "nice\n",
      "guy\n",
      "new\n",
      "email\n",
      "woman\n",
      "never\n",
      "guys\n",
      "movie\n",
      "home\n",
      "death\n",
      "info\n",
      "interested\n",
      "exploring\n",
      "gym\n",
      "show\n",
      "good\n",
      "little\n",
      "see\n",
      "married\n",
      "lady\n",
      "meet\n",
      "try\n",
      "outdoors\n",
      "female\n",
      "actually\n",
      "rut\n",
      "snuggling\n",
      "quirk\n",
      "kind\n",
      "prints\n",
      "notes\n",
      "buy\n",
      "still\n",
      "girls\n",
      "hear\n",
      "bored\n",
      "marriage\n",
      "bottom\n",
      "showcontact\n",
      "class\n",
      "blaze\n",
      "step\n",
      "sleep\n",
      "haha\n",
      "year\n",
      "bit\n",
      "hot\n",
      "athletic\n",
      "vacation\n",
      "straight\n",
      "guess\n",
      "know\n",
      "general\n",
      "variety\n",
      "pics\n",
      "hard\n",
      "again\n",
      "stuck\n",
      "wide\n",
      "blankets\n",
      "vajayjays\n",
      "under\n",
      "african\n",
      "curious\n",
      "been\n",
      "mind\n",
      "flirty\n",
      "change\n",
      "trade\n",
      "before\n",
      "preference\n",
      "answer\n",
      "compare\n",
      "nothing\n",
      "loves\n",
      "click\n",
      "bed\n",
      "watch\n",
      "tell\n",
      "one\n",
      "old\n",
      "bare\n",
      "could\n",
      "through\n",
      "realm\n",
      "kik\n",
      "while\n",
      "stats\n",
      "longer\n",
      "animals\n",
      "smartass\n",
      "even\n",
      "open\n",
      "art\n",
      "meal\n",
      "tonight\n",
      "hello\n",
      "threesome\n",
      "snuggled\n",
      "might\n",
      "friends\n",
      "chris\n",
      "sfo\n",
      "lookin\n",
      "hope\n",
      "post\n",
      "buddy\n",
      "hey\n",
      "male\n",
      "games\n",
      "whether\n",
      "believes\n",
      "perhaps\n",
      "host\n",
      "fantasies\n",
      "person\n",
      "please\n",
      "has\n",
      "stp\n",
      "upbeat\n",
      "extra\n",
      "work\n",
      "playing\n",
      "eventually\n",
      "motivate\n",
      "own\n",
      "possibly\n",
      "tall\n",
      "today\n",
      "when\n",
      "cocks\n",
      "sane\n",
      "call\n",
      "honest\n",
      "here\n",
      "kick\n",
      "online\n",
      "hitting\n",
      "healthy\n",
      "minded\n",
      "different\n",
      "going\n",
      "erotic\n",
      "viewing\n",
      "spanking\n",
      "bring\n",
      "smoking\n",
      "photo\n",
      "somethin\n",
      "think\n",
      "scenarios\n",
      "goes\n",
      "pieces\n",
      "hip\n",
      "too\n",
      "enhancing\n",
      "hung\n",
      "whateve\n",
      "say\n",
      "sounds\n",
      "enjoys\n",
      "motivation\n",
      "boat\n",
      "attention\n",
      "then\n",
      "discuss\n",
      "free\n",
      "4939032675\n",
      "night\n",
      "buddies\n",
      "interests\n",
      "collecting\n",
      "first\n",
      "where\n",
      "hypnosis\n",
      "american\n",
      "hookie\n",
      "companion\n",
      "life\n",
      "selling\n",
      "thanks\n",
      "warm\n",
      "session\n",
      "reading\n",
      "motions\n",
      "mediums\n",
      "anyone\n",
      "bbw\n",
      "buying\n",
      "mexican\n",
      "goddess\n",
      "drug\n",
      "off\n",
      "way\n",
      "href\n",
      "than\n",
      "distraction\n",
      "willing\n",
      "something\n",
      "any\n",
      "roleplay\n",
      "san\n",
      "fashioned\n",
      "help\n",
      "phone\n",
      "128522\n",
      "don\n",
      "says\n",
      "same\n",
      "times\n",
      "age\n",
      "look\n",
      "originals\n",
      "live\n",
      "endless\n",
      "leandro\n",
      "lol\n",
      "day\n",
      "emails\n",
      "thank\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "will\n",
      "her\n",
      "interesting\n",
      "been\n",
      "plus\n",
      "had\n",
      "anyone\n",
      "live\n",
      "female\n",
      "clean\n",
      "would\n",
      "eating\n",
      "hot\n",
      "know\n",
      "right\n",
      "maybe\n",
      "personal\n",
      "conversation\n",
      "mind\n",
      "before\n",
      "smoke\n",
      "prefer\n",
      "into\n",
      "friends\n",
      "feel\n",
      "has\n",
      "work\n",
      "chill\n",
      "size\n",
      "disorders\n",
      "also\n",
      "book\n",
      "massage\n",
      "long\n",
      "than\n",
      "any\n",
      "good\n",
      "don\n",
      "either\n",
      "day\n",
      "breasts\n",
      "relaxing\n",
      "kind\n",
      "week\n",
      "obsession\n",
      "older\n",
      "mid\n",
      "three\n",
      "weekdays\n",
      "hours\n",
      "water\n",
      "make\n",
      "reg\n",
      "special\n",
      "curse\n",
      "needed\n",
      "year\n",
      "bit\n",
      "boyfriend\n",
      "ive\n",
      "once\n",
      "music\n",
      "did\n",
      "sports\n",
      "diagnosed\n",
      "lactating\n",
      "insight\n",
      "city\n",
      "non\n",
      "seeking\n",
      "nights\n",
      "milk\n",
      "simple\n",
      "years\n",
      "give\n",
      "opposed\n",
      "days\n",
      "since\n",
      "dreaming\n",
      "white\n",
      "two\n",
      "answer\n",
      "anymore\n",
      "nothing\n",
      "fiction\n",
      "relationship\n",
      "send\n",
      "bumb\n",
      "women\n",
      "them\n",
      "posting\n",
      "else\n",
      "through\n",
      "foot\n",
      "return\n",
      "during\n",
      "island\n",
      "safe\n",
      "kik\n",
      "fit\n",
      "tattoos\n",
      "sbm\n",
      "deal\n",
      "even\n",
      "open\n",
      "bulimia\n",
      "talk\n",
      "man\n",
      "provide\n",
      "slim\n",
      "50mwm\n",
      "handsome\n",
      "drugs\n",
      "mist\n",
      "etc\n",
      "clothes\n",
      "ignored\n",
      "knows\n",
      "quick\n",
      "dont\n",
      "wanna\n",
      "couple\n",
      "hope\n",
      "drink\n",
      "blk\n",
      "pampered\n",
      "hey\n",
      "adult\n",
      "host\n",
      "love\n",
      "regular\n",
      "museums\n",
      "dirty\n",
      "suckling\n",
      "currently\n",
      "m4w\n",
      "alcohol\n",
      "please\n",
      "side\n",
      "words\n",
      "treat\n",
      "basis\n",
      "offer\n",
      "business\n",
      "wouldn\n",
      "nutritious\n",
      "plays\n",
      "outside\n",
      "bronx\n",
      "tall\n",
      "are\n",
      "title\n",
      "when\n",
      "pls\n",
      "learning\n",
      "assistant\n",
      "upstate\n",
      "laying\n",
      "laugh\n",
      "provided\n",
      "working\n",
      "piercings\n",
      "enjoy\n",
      "having\n",
      "ilovegabby3150\n",
      "especially\n",
      "doesn\n",
      "nice\n",
      "wants\n",
      "new\n",
      "different\n",
      "m4mw\n",
      "treatment\n",
      "tub\n",
      "email\n",
      "dudes\n",
      "very\n",
      "individuals\n",
      "trimester\n",
      "hour\n",
      "sold\n",
      "woman\n",
      "ltr\n",
      "kids\n",
      "these\n",
      "hmu\n",
      "never\n",
      "now\n",
      "given\n",
      "owl\n",
      "young\n",
      "supply\n",
      "anykind\n",
      "amazing\n",
      "particularly\n",
      "figuring\n",
      "making\n",
      "thirties\n",
      "second\n",
      "should\n",
      "hav\n",
      "watching\n",
      "discrete\n",
      "discuss\n",
      "steam\n",
      "pleasure\n",
      "night\n",
      "whole\n",
      "consistent\n",
      "worshiped\n",
      "tired\n",
      "everything\n",
      "fine\n",
      "beautiful\n",
      "marie\n",
      "home\n",
      "tea\n",
      "read\n",
      "where\n",
      "studying\n",
      "anorexia\n",
      "interested\n",
      "life\n",
      "hanging\n",
      "warm\n",
      "ago\n",
      "cut\n",
      "pay\n",
      "womans\n",
      "better\n",
      "area\n",
      "appearance\n",
      "crossdresser\n",
      "residing\n",
      "pregnancy\n",
      "m4ww\n",
      "shared\n",
      "lick\n",
      "release\n",
      "something\n",
      "sweet\n",
      "stimulated\n",
      "caucasian\n",
      "decent\n",
      "start\n",
      "tenderness\n",
      "place\n",
      "haven\n",
      "help\n",
      "little\n",
      "always\n",
      "clubs\n",
      "share\n",
      "breast\n",
      "says\n",
      "see\n",
      "which\n",
      "best\n",
      "come\n",
      "job\n",
      "men\n",
      "how\n",
      "soft\n",
      "let\n",
      "stress\n",
      "mature\n",
      "including\n",
      "heels\n",
      "lady\n",
      "necessarily\n",
      "improving\n",
      "late\n",
      "meet\n",
      "they\n",
      "anybody\n",
      "she\n",
      "getting\n",
      "experience\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hwayo/.pyenv/versions/machinelearning/lib/python3.4/site-packages/IPython/kernel/__main__.py:33: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/hwayo/.pyenv/versions/machinelearning/lib/python3.4/site-packages/IPython/kernel/__main__.py:34: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/hwayo/.pyenv/versions/machinelearning/lib/python3.4/site-packages/IPython/kernel/__main__.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "/Users/hwayo/.pyenv/versions/machinelearning/lib/python3.4/site-packages/IPython/kernel/__main__.py:11: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_top_words(ny, sf):\n",
    "    import operator\n",
    "    \n",
    "    # 나이스베이스 분류기를 훈련하고 검사한다. 이때 사용된 확률은 반환된다.\n",
    "    vocab_list, p_0_v, p_1_v = local_words(ny, sf)\n",
    "    \n",
    "    # 그런다음 두개의 리스트를 생성한다.\n",
    "    top_NY = []\n",
    "    top_SF = []\n",
    "    \n",
    "    # 리스트 내부는 튜플 형태로 저장한다.\n",
    "    # 상위 X개의 단어를 반환하는 것보다 특정 임계 값 이상인 단어를 모두 반환하는 것이 좋다.\n",
    "    for i in range(len(p_0_v)):\n",
    "        if p_0_v[i] > -6.0:\n",
    "            top_SF.append((vocab_list[i], p_0_v[i]))\n",
    "        if p_1_v[i] > -6.0:\n",
    "            top_NY.append((vocab_list[i], p_1_v[i]))\n",
    "    \n",
    "    sorted_SF = sorted(top_SF, key=lambda pair: pair[1], reverse=True)\n",
    "    \n",
    "    print (\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n",
    "    \n",
    "    for item in sorted_SF:\n",
    "        print (item[0])\n",
    "    \n",
    "    sorted_NY = sorted(top_NY, key=lambda pair: pair[1], reverse=True)\n",
    "    \n",
    "    print (\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\")\n",
    "    for item in sorted_NY:\n",
    "        print (item[0])\n",
    "        \n",
    "# Test\n",
    "\n",
    "get_top_words(ny, sf)\n",
    "\n",
    "# 유의사항\n",
    "# 출력된 단어에 중지 단어가 많이 나타난다는 것이다. 미리 선정한 중지 단어를 제거하고,\n",
    "# 결과가 어떻게 변화하는지 확인하는 것도 흥미로울 것이다.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "베이지안 확률과 베이스 규칙은 우리에게 알려진 값으로부터 알려지지 않은 것의 확률을 추정하는 방법을 제공한다.\n",
    "\n",
    "데이터 속성 간의 관계가 조건부 독립이라고 가정함으로써 필요한 데이터의 양을 많이 줄일 수 있다.\n",
    "\n",
    "우리가 세운 가정은 문서에서 한 단어의 확률이 다른 어떠한 단어에도 의존하지 않는다는 것이다.\n",
    "(이것은 나이브 베이스로 알려진 방법이다.(\n",
    "잘못된 가정에도 불구하고 나이브 베이스는 분류에 효과적이다.\n",
    "\n",
    "언더플로우 문제 계산을 할 때 확률에 로그를 사용함으로써 해결할 수 있다.\n",
    "문서 분류 처리에 있어서 중복 단어 모델은 집합 단어 모델을 개선한 것이다.\n",
    "중지 단어를 삭제하는 것처럼 개선하는 방법도 여러가지가 있다.\n",
    "당신은 토큰을 최적화 하는데 많은 시간을 보내게 될 것이다.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
